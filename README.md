# End-to-End-Db-workflow
This project demonstrates a complete machine learning workflow built around the Titanic datasetâ€”from pulling data from a MySQL database, preprocessing and feature engineering, building a LightGBM classification model inside a pipeline, and saving the model's predictions back into the database.

ğŸ“Œ Project Overview

Goal: Predict passenger survival on the Titanic using a custom ML pipeline and store results in a MySQL table for future use or dashboard integration.

âœ… Key Components

Data Source: Titanic dataset pulled from a MySQL database

Preprocessing: Custom feature engineering using FunctionTransformer

Modeling: LightGBM Classifier with RandomizedSearchCV for hyperparameter tuning

Integration: Writes prediction results back into a new or existing table in MySQL

ğŸ—ï¸ Project Structure

titanic_ml_project/
â”‚
â”œâ”€â”€ db_pull.py              # Loads Titanic data from MySQL
â”œâ”€â”€ features.py             # Contains custom feature engineering functions
â”œâ”€â”€ model_pipeline.py       # Builds and trains the ML pipeline
â”œâ”€â”€ save_predictions.py     # Pushes predictions to a new/existing DB table
â”œâ”€â”€ .env                    # Stores DB credentials (never push to GitHub)
â”œâ”€â”€ README.md               # Project documentation

âš™ï¸ How It Works

1. ğŸ”Œ Data Connection

Connects to a local or remote MySQL database using credentials from .env and loads the titanic_train table into a pandas DataFrame.

from db_pull import get_data

df = get_data()

2. ğŸ§  Feature Engineering

Adds two new columns:

length_of_name: Length of the passenger's full name.

is_a_minor: Flags passengers under 18 as minors.

from features import add_features
df = add_features(df)

3. ğŸ§² Model Pipeline

Uses FunctionTransformer to add features inside a pipeline.

Fits an LGBMClassifier wrapped inside RandomizedSearchCV for optimization.

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer
from lightgbm import LGBMClassifier

4. ğŸ“‚ Save Predictions

The test set predictions are added as a new column (prediction) and written to a new MySQL table (titanic_with_predictions).

from save_predictions import save_predictions

save_predictions(df_with_preds, table_name='titanic_with_predictions')

ğŸ’  Requirements

pandas
sqlalchemy
pymysql
scikit-learn
lightgbm
python-dotenv

Install all dependencies using:

pip install -r requirements.txt

ğŸ§ª Example Run Flow

Setup your .env with DB credentials:

db_username=root
db_password=yourpassword
db_host=localhost
db_port=3306
db_database=your_database

Run in order:

python model_pipeline.py

ğŸ“Š Output Example

Saved table in MySQL (titanic_with_predictions) includes:

PassengerId

Pclass

Sex

Age

Fare

Embarked

prediction

1

3

male

22

7.25

S

0

2

1

female

38

71.3

C

1

ğŸ“ Future Improvements

Add more complex feature engineering (e.g. family size, cabin prefix).

Export the trained model using joblib.

Add a Flask or FastAPI interface for real-time inference.

ğŸ§‘â€ğŸ’» Author

Emmanuel Nwankwo Ota - Machine Learning Engineer | Real-world AI & database integration enthusiastBuilt with â¤ï¸ using Python, MySQL, and scikit-learn

